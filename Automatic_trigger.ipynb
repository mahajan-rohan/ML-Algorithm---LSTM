{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin #type:ignore\n",
    "from firebase_admin import credentials, firestore #type:ignore\n",
    "import pandas as pd #type:ignore\n",
    "import numpy as np #type:ignore\n",
    "import tensorflow as tf #type:ignore\n",
    "from tensorflow.keras.models import Sequential #type:ignore\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout #type:ignore\n",
    "from sklearn.preprocessing import MinMaxScaler #type:ignore\n",
    "import matplotlib.pyplot as plt #type:ignore\n",
    "\n",
    "# Step 1: Initialize Firebase Admin SDK (This needs to be done only once)\n",
    "try:\n",
    "    cred = credentials.Certificate(\"path/to/your-firebase-adminsdk.json\")  # Replace with your Firebase credentials file\n",
    "    firebase_admin.initialize_app(cred)\n",
    "except:\n",
    "    print(\"Firebase already initialized or failed to initialize\")\n",
    "\n",
    "# Step 2: Fetch Data from Firebase Firestore\n",
    "db = firestore.client()\n",
    "data_ref = db.collection('your_collection_name')  # Replace with your Firebase collection name\n",
    "\n",
    "# Fetch data\n",
    "docs = data_ref.stream()\n",
    "\n",
    "# Store the timestamp and energy values in a list\n",
    "data = []\n",
    "for doc in docs:\n",
    "    timestamp = doc.get('timestamp')  # Ensure your Firestore document contains 'timestamp' field\n",
    "    energy = doc.get('energy')        # Ensure your Firestore document contains 'energy' field\n",
    "    data.append({'timestamp': timestamp, 'energy': energy})\n",
    "\n",
    "# Convert the data to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Step 3: Prepare the data for LSTM\n",
    "energy_data = df[['energy']].values\n",
    "\n",
    "# Feature scaling (Normalization)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "energy_scaled = scaler.fit_transform(energy_data)\n",
    "\n",
    "# Create sequences for LSTM (look-back window)\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(seq_length, len(data)):\n",
    "        sequences.append(data[i-seq_length:i, 0])\n",
    "        labels.append(data[i, 0])\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "SEQ_LENGTH = 60  # You can adjust this for how many past data points to consider\n",
    "X, y = create_sequences(energy_scaled, SEQ_LENGTH)\n",
    "\n",
    "# Reshape for LSTM input (samples, timesteps, features)\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "\n",
    "# Step 4: Build LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(X.shape[1], 1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=50, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Step 5: Train the model\n",
    "history = model.fit(X, y, epochs=20, batch_size=32)\n",
    "\n",
    "# Step 6: Plot the training loss (optional)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.title('Model Loss During Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 7: Predictions (optional)\n",
    "predictions = model.predict(X)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "# Show prediction for next step\n",
    "print(\"Predicted energy for next timestep:\", predictions[-1])\n",
    "\n",
    "# Optional: Plot actual vs predicted (to visualize performance)\n",
    "plt.plot(df.index[SEQ_LENGTH:], energy_data[SEQ_LENGTH:], label='True Energy Data', color='blue')\n",
    "plt.plot(df.index[SEQ_LENGTH:], predictions, label='Predicted Energy Data', color='red')\n",
    "plt.title('Actual vs Predicted Energy')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Energy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential #type:ignore\n",
    "from tensorflow.keras.layers import LSTM, Dense #type:ignore\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('Datasets/data.csv')\n",
    "\n",
    "# Convert 'timestamp' to datetime format\n",
    "data['timestamp'] = pd.to_datetime(data['timestamp'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "\n",
    "# Set the 'timestamp' column as the index for time-series modeling\n",
    "data.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Ensure 'unitConsumption' column is numeric; convert if necessary\n",
    "data['unitConsumption'] = pd.to_numeric(data['unitConsumption'], errors='coerce')\n",
    "\n",
    "# Handle any missing values created during conversion\n",
    "data.fillna(method='ffill', inplace=True)  # Forward fill for missing values\n",
    "\n",
    "# Normalize the data using MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data['unitConsumption'] = scaler.fit_transform(data['unitConsumption'].values.reshape(-1, 1))\n",
    "\n",
    "# Prepare the dataset for LSTM\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - time_step - 1):\n",
    "        a = data[i:(i + time_step)]  # Corrected: Now accessing the data properly\n",
    "        X.append(a)\n",
    "        Y.append(data[i + time_step])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "# Specify the time step (window size)\n",
    "time_step = 10  # Can be adjusted depending on how much past data you want to use for prediction\n",
    "X, y = create_dataset(data['unitConsumption'].values, time_step)\n",
    "\n",
    "# Reshape the data for LSTM [samples, timesteps, features]\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(X_train.shape[1], 1), activation='relu'))\n",
    "model.add(Dense(1))  # Output layer for regression\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse transform the predictions and the actual test data back to the original scale\n",
    "y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "y_pred = scaler.inverse_transform(y_pred)\n",
    "\n",
    "# Save predictions and actual values to a JSON file\n",
    "output_data = {\n",
    "    \"actual\": y_test.flatten().tolist(),\n",
    "    \"predicted\": y_pred.flatten().tolist()\n",
    "}\n",
    "\n",
    "# Specify the output file path\n",
    "output_file = 'output.json'\n",
    "\n",
    "# Write to JSON file\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(output_data, f, indent=4)\n",
    "\n",
    "# Print success message\n",
    "print(f\"Predictions and actual values saved to {output_file}\")\n",
    "\n",
    "# Optionally, plot the actual vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_pred, label='Predicted unitConsumption', marker='o')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('unitConsumption')\n",
    "plt.title('unitConsumption: Actual vs. Predicted')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic trigger on specific time code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from apscheduler.schedulers.blocking import BlockingScheduler\n",
    "import datetime\n",
    "\n",
    "# Define the ML algorithm function\n",
    "def run_ml_algorithm():\n",
    "    print(\"Running ML algorithm at\", datetime.datetime.now())\n",
    "\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv('Datasets/data.csv')\n",
    "\n",
    "    # Convert 'timestamp' to datetime format\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "\n",
    "    # Set the 'timestamp' column as the index for time-series modeling\n",
    "    data.set_index('timestamp', inplace=True)\n",
    "\n",
    "    # Ensure 'unitConsumption' column is numeric; convert if necessary\n",
    "    data['unitConsumption'] = pd.to_numeric(data['unitConsumption'], errors='coerce')\n",
    "\n",
    "    # Handle any missing values created during conversion\n",
    "    data.fillna(method='ffill', inplace=True)  # Forward fill for missing values\n",
    "\n",
    "    # Normalize the data using MinMaxScaler\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data['unitConsumption'] = scaler.fit_transform(data['unitConsumption'].values.reshape(-1, 1))\n",
    "\n",
    "    # Prepare the dataset for LSTM\n",
    "    def create_dataset(data, time_step=1):\n",
    "        X, Y = [], []\n",
    "        for i in range(len(data) - time_step - 1):\n",
    "            a = data[i:(i + time_step)]  # Corrected: Now accessing the data properly\n",
    "            X.append(a)\n",
    "            Y.append(data[i + time_step])\n",
    "        return np.array(X), np.array(Y)\n",
    "\n",
    "    # Specify the time step (window size)\n",
    "    time_step = 10  # Can be adjusted depending on how much past data you want to use for prediction\n",
    "    X, y = create_dataset(data['unitConsumption'].values, time_step)\n",
    "\n",
    "    # Reshape the data for LSTM [samples, timesteps, features]\n",
    "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(X_train.shape[1], 1), activation='relu'))\n",
    "    model.add(Dense(1))  # Output layer for regression\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Inverse transform the predictions and the actual test data back to the original scale\n",
    "    y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "    y_pred = scaler.inverse_transform(y_pred)\n",
    "\n",
    "    # Save predictions and actual values to a JSON file\n",
    "    output_data = {\n",
    "        \"actual\": y_test.flatten().tolist(),\n",
    "        \"predicted\": y_pred.flatten().tolist()\n",
    "    }\n",
    "\n",
    "    # Specify the output file path\n",
    "    output_file = 'output.json'\n",
    "\n",
    "    # Write to JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(output_data, f, indent=4)\n",
    "\n",
    "    # Print success message\n",
    "    print(f\"Predictions and actual values saved to {output_file}\")\n",
    "\n",
    "    # Optionally, plot the actual vs predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(y_pred, label='Predicted unitConsumption', marker='o')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('unitConsumption')\n",
    "    plt.title('unitConsumption: Actual vs. Predicted')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    scheduler.shutdown(wait=False)\n",
    "# Create an instance of the scheduler\n",
    "scheduler = BlockingScheduler()\n",
    "\n",
    "# Schedule the ML algorithm at 10 PM daily\n",
    "scheduler.add_job(run_ml_algorithm, 'cron', hour=21, minute=52)\n",
    "\n",
    "# Start the scheduler\n",
    "try:\n",
    "    scheduler.start()\n",
    "except (KeyboardInterrupt, SystemExit):\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
